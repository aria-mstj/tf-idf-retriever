{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhhOulic3Fko"
      },
      "source": [
        "# 1- Preprocess\n",
        "At first for preprocessing our data, we should read it. I read it from local computer, you can find it on my git."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "KuQ6ujwR25Yw",
        "outputId": "8a646b2c-3bd2-4eff-d862-7347dbd61c09"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2cfefd97-5f97-4444-92c5-9fda01b32956\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2cfefd97-5f97-4444-92c5-9fda01b32956\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving IR1_7k_news.xlsx to IR1_7k_news.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "FgG1KfMB3lS-",
        "outputId": "9d0a6f7a-dca1-40ef-8482-5155373ce5cb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(r'IR1_7k_news.xlsx')\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>مسلم مجدمی در گفت‌وگو با خبرنگار ورزشی خبرگزار...</td>\n",
              "      <td>https://www.farsnews.ir/news/14000803000922/وا...</td>\n",
              "      <td>واکنش بازیکن نفت مسجدسلیمان به صحبت‌های پیروان...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>به گزارش خبرنگار ورزشی خبرگزاری فارس، در آخرین...</td>\n",
              "      <td>https://www.farsnews.ir/news/14000803000896/هف...</td>\n",
              "      <td>هفته دوم لیگ برتر| پرسپولیس مقابل تیم سختکوش ن...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>به گزارش خبرنگار ورزشی خبرگزاری فارس، زندگی بد...</td>\n",
              "      <td>https://www.farsnews.ir/news/14000803000413/اج...</td>\n",
              "      <td>اجاره نشینی قهرمان جهان و پارالمپیک در خانه 50...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>پرویز برومند در گفت و گو با خبرنگار ورزشی خبرگ...</td>\n",
              "      <td>https://www.farsnews.ir/news/14000803000926/بر...</td>\n",
              "      <td>برومند: یامگا هم می تواند جباری باشد هم تیام/ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>به گزارش خبرگزاری فارس، فهرستی از برترین مهاجم...</td>\n",
              "      <td>https://www.farsnews.ir/news/14000803000971/بر...</td>\n",
              "      <td>برترین مهاجمان فوتبال با بهترین میانگین گلزنی/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7557</th>\n",
              "      <td>به گزارش ایسنا، این بازیگر همچنین عنوان کرد که...</td>\n",
              "      <td>https://www.isna.ir/news/00061797016/ماجرای-فک...</td>\n",
              "      <td>ماجرای فَک خانم بازیگر به کجا انجامید؟</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7558</th>\n",
              "      <td>به گزارش ایسنا، کوروش سلیمانی بازیگر سینما، تئ...</td>\n",
              "      <td>https://www.isna.ir/news/00061796940/برترین-سر...</td>\n",
              "      <td>برترین سریال‌های جاسوسی ـ امنیتی تلویزیون به ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7559</th>\n",
              "      <td>به گزارش ایسنا به نقل از روابط عمومی شبکه دو، ...</td>\n",
              "      <td>https://www.isna.ir/news/1400073021743/کاوه-آف...</td>\n",
              "      <td>کاوه آفاق برای «رخ به رخ» تلویزیون می‌خواند</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7560</th>\n",
              "      <td>به گزارش ایسنا، ساجد قدوسیان تهیه‌کننده، سردبی...</td>\n",
              "      <td>https://www.isna.ir/news/1400073021738/صبح-جمع...</td>\n",
              "      <td>«صبح جمعه با شما» زنده پخش می‌شود</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7561</th>\n",
              "      <td>به گزارش ایسنا به نقل از مرکز روابط عمومی و اط...</td>\n",
              "      <td>https://www.isna.ir/news/1400073021719/پیام-تس...</td>\n",
              "      <td>پیام تسلیت وزیر ارشاد برای درگذشت حافظ لالایی‌...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7562 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                content  ...                                              title\n",
              "0     مسلم مجدمی در گفت‌وگو با خبرنگار ورزشی خبرگزار...  ...  واکنش بازیکن نفت مسجدسلیمان به صحبت‌های پیروان...\n",
              "1     به گزارش خبرنگار ورزشی خبرگزاری فارس، در آخرین...  ...  هفته دوم لیگ برتر| پرسپولیس مقابل تیم سختکوش ن...\n",
              "2     به گزارش خبرنگار ورزشی خبرگزاری فارس، زندگی بد...  ...  اجاره نشینی قهرمان جهان و پارالمپیک در خانه 50...\n",
              "3     پرویز برومند در گفت و گو با خبرنگار ورزشی خبرگ...  ...  برومند: یامگا هم می تواند جباری باشد هم تیام/ا...\n",
              "4     به گزارش خبرگزاری فارس، فهرستی از برترین مهاجم...  ...  برترین مهاجمان فوتبال با بهترین میانگین گلزنی/...\n",
              "...                                                 ...  ...                                                ...\n",
              "7557  به گزارش ایسنا، این بازیگر همچنین عنوان کرد که...  ...             ماجرای فَک خانم بازیگر به کجا انجامید؟\n",
              "7558  به گزارش ایسنا، کوروش سلیمانی بازیگر سینما، تئ...  ...  برترین سریال‌های جاسوسی ـ امنیتی تلویزیون به ا...\n",
              "7559  به گزارش ایسنا به نقل از روابط عمومی شبکه دو، ...  ...        کاوه آفاق برای «رخ به رخ» تلویزیون می‌خواند\n",
              "7560  به گزارش ایسنا، ساجد قدوسیان تهیه‌کننده، سردبی...  ...                  «صبح جمعه با شما» زنده پخش می‌شود\n",
              "7561  به گزارش ایسنا به نقل از مرکز روابط عمومی و اط...  ...  پیام تسلیت وزیر ارشاد برای درگذشت حافظ لالایی‌...\n",
              "\n",
              "[7562 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHBFep2Yv8dV"
      },
      "source": [
        "Now we have our dataset. For preprocess we should tokenize, normalize, remove stop words and at the end stem our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br2ZN59q-1cS",
        "outputId": "27f9f5fc-c589-4b7c-f3d0-a0453ea7779e"
      },
      "source": [
        "docs = dict(zip(list(df['title']), list(df['content'])))\n",
        "items = docs.items()\n",
        "list(items)[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('واکنش بازیکن نفت مسجدسلیمان به صحبت\\u200cهای پیروانی:با شرافت بازی کردیم/ این حرف ها زشت است',\n",
              " 'مسلم مجدمی در گفت\\u200cوگو با خبرنگار ورزشی خبرگزاری فارس در مورد شکست 3 بر صفر نفت\\nمسجدسلیمان مقابل سپاهان اظهار داشت: فکر نمی\\u200cکردیم این اتفاق بیفتد. زود گل\\nخوردیم و غافلگیر شدیم. پیش از شروع مسابقات تنها یک هفته تمرین کرده بودیم و با\\nاین شرایط نمی\\u200cتوان تیم ما را با بقیه تیم\\u200cها مقایسه کرد. برخی تیم\\u200cها حتی بیشتر\\nاز 10 بازی تدارکاتی انجام دادند و بدنسازی به موقعی داشتند.\\n\\nوی تصریح کرد: پیش از این بازی هم وضعیت\\u200c ما مشخص نبود. در زمین خودمان بازی\\nنمی\\u200cکردیم و همه این اتفاقات دست به دست هم داد مشکلات زیادی داشته باشیم. با این\\nحال فوتبال برد و باخت دارد و تلاش می\\u200cکنیم تا به امید خدا این نتیجه را در\\nبازی\\u200cهای بعدی جبران کنیم.\\n\\nمجدمی در مورد زیر کشت رفتن و شخم زدن زمین ورزشگاه بهنام پیش از بازی با سپاهان\\nعنوان کرد:\\u200c ما برای تمرین به مسجدسلیمان رفتیم اما دیدیم این اتفاق برای زمین\\nافتاده است. نمی\\u200cدانم کار چه کسی بوده اما واقعاً به ما ضربه زد. راه زیادی را از\\nتهران به اهواز و سپس به مسجدسلیمان رفتیم. می\\u200cخواستیم آماده بازی با سپاهان شویم\\nاما دیدیم شرایط زمین خوب نیست. اگر در آن تمرین هم می\\u200cکردیم بدتر می\\u200cشد. حتی یک\\nجلسه هم پشت دروازه تمرین کردیم اما مجبور شدیم به اهواز برگردیم. با این شرایط\\nنمی\\u200cدانم بعد از چند هفته زمین خانگی ما آماده مسابقه می\\u200cشود.\\n\\nهافبک تیم فوتبال نفت مسجدسلیمان در مورد اعتراض [باشگاه\\nپرسپولیس](https://www.farsnews.ir/special/باشگاه پرسپولیس) به تغییر محل زمین\\nبازی و اینکه افشین پیروانی اعلام کرد این اتفاق برای دادن امتیاز به تیم سپاهان\\nبود، گفت: فکر نمی\\u200cکنم در این شرایط تیمی بخواهد به تیم دیگری امتیاز بدهد. این\\nحرف\\u200cها زشت است. بچه\\u200cهای ما با\\n[شرافت](https://search.farsnews.ir/?q=شرافت&o=on) بازی کردند. باید چه کار\\nمی\\u200cکردیم؟ مجبور بودیم برای بازی به اهواز برویم. کجا باید بازی می\\u200cکردیم؟ ما هم\\nدوست داشتیم در این شرایط از میزبانی خودمان استفاده کنیم. در هفته اول یک امتیاز\\nاز پیکان گرفتیم و در بازی با سپاهان هم اگر حتی یک امتیاز می\\u200cگرفتیم نتیجه\\n[خوبی](https://search.farsnews.ir/?q=خوبی&o=on) بود. این حرف\\u200cها برای یک باشگاه\\nفوتبال قشنگ نیست.\\n\\nمجدمی در مورد استعفای مدیرعامل باشگاه نفت مسجدسلیمان اظهار داشت: من از این\\nمسئله مطمئن نیستم. دیروز عصر سر تمرین هم خبری نبود اما صددرصد هرچه تغییرات\\nزیاد باشد تیم را اذیت می\\u200cکند. ما تابع تصمیمات باشگاه هستیم و امیدوارم هر\\nاتفاقی می\\u200cافتد به صلاح تیم باشد.\\n\\nوی در مورد بازی هفته آینده با تیم بحران زده پدیده در مشهد، گفت: قطعاً در این\\nبازی تلاش می\\u200cکنیم که بتوانیم نتیجه بگیریم. تیم خوبی داریم و باید به خودمان\\nنگاه کنیم. مقابل پیکان بازی خوبی انجام دادیم. مقابل سپاهان هم نباید اینطور\\nشکست می\\u200cخوردیم اما غافلگیر شدیم. پدیده هم شرایط خوبی ندارد و بعضی اوقات تعویض\\nمربیان به نفع حریف می\\u200cشود. در هر حال تمام تلاش\\u200cمان این است که با دست پر از\\nمشهد برگردیم.\\n\\nانتهای پیام/\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hz5A5CcxBtb"
      },
      "source": [
        "For preprocess, I used [parsivar](https://github.com/ICTRC/Parsivar) library. First step is to normalize our data. Normalizing data means changing data to processable data. For example changing ۱ to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeebM0S_wYSM",
        "outputId": "53881976-25cb-4780-9a34-99fa2ec8978a"
      },
      "source": [
        "!pip install parsivar"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting parsivar\n",
            "  Downloading parsivar-0.2.3.tar.gz (36.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.2 MB 316 kB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 26.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->parsivar) (1.15.0)\n",
            "Building wheels for collected packages: parsivar, nltk\n",
            "  Building wheel for parsivar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsivar: filename=parsivar-0.2.3-py3-none-any.whl size=36492971 sha256=90d659223765ea087cbd89443ecad8c5447de8f828ac27d88b845e8ef80b2c9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/67/7a/49cbf08f64d3f76a26eceaf0e481a40e233f05d4356875cbed\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=93ff8fd996010ab2df8e83c76e682f6c24d7398f9b090d9faf2c69f0cda82ab4\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "Successfully built parsivar nltk\n",
            "Installing collected packages: nltk, parsivar\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5 parsivar-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsNGeDCDFtSQ"
      },
      "source": [
        "For checking heaps' law we need to do a small tokenization before our final tokenization and check if before removing stop words is it true or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTZW1iqbFs7x",
        "outputId": "6f380d9b-77fc-4178-ccc5-87d88f0038e3"
      },
      "source": [
        "def f(v):\n",
        "  return word_tokenize(v)\n",
        "\n",
        "tmp_tokenized_docs = {k: f(v) for k, v in docs.items()}\n",
        "tmp_all_dict = {}\n",
        "\n",
        "tmp_data_set = {}\n",
        "for doc in tmp_tokenized_docs.values():\n",
        "  uniqued_list = list(dict.fromkeys(doc))\n",
        "  unique_doc = {}\n",
        "  for word in uniqued_list:\n",
        "    unique_doc[word] = doc.count(word)\n",
        "  tmp_data_set[list(tmp_tokenized_docs.keys())[list(tmp_tokenized_docs.values()).index(doc)]] = unique_doc\n",
        "\n",
        "# tmp_bigdataset = {}\n",
        "for postings_list in tmp_data_set.values():\n",
        "  for word in postings_list.keys():\n",
        "    if word in tmp_all_dict.keys():\n",
        "      tmp_all_dict[word] += postings_list[word]\n",
        "    else:\n",
        "      tmp_all_dict[word] = postings_list[word]\n",
        "T = len(tmp_all_dict.values())\n",
        "print(\"Number of T : \", T)\n",
        "print(\"b = 1/2 and k = 30 : \", 30 * T ** 0.5)\n",
        "print(\"b = 1/2 and k = 100 : \", 100 * T ** 0.5)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of T :  72714\n",
            "b = 1/2 and k = 30 :  8089.66006702383\n",
            "b = 1/2 and k = 100 :  26965.5335567461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXG7p0g-G-K_"
      },
      "source": [
        "We know that the real size of our data is equal to 7992kb so the heaps' law is true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYThDeV1yTAD"
      },
      "source": [
        "from parsivar import Normalizer\n",
        "my_normalizer = Normalizer()\n",
        "# pp_docs = [my_normalizer.normalize(txt) for txt in docs] #preprocessed docs\n",
        "def f(v):\n",
        "  return my_normalizer.normalize(v)\n",
        "pp_docs = {k: f(v) for k, v in docs.items()}\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK6KClljHcjW"
      },
      "source": [
        "Now let's do heap's law again "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxrUfRD1H5VW",
        "outputId": "9b9a20cc-97d9-41fd-e898-819f44d067df"
      },
      "source": [
        "def f(v):\n",
        "  return word_tokenize(v)\n",
        "\n",
        "tmp_tokenized_docs = {k: f(v) for k, v in pp_docs.items()}\n",
        "tmp_all_dict = {}\n",
        "\n",
        "tmp_data_set = {}\n",
        "for doc in tmp_tokenized_docs.values():\n",
        "  uniqued_list = list(dict.fromkeys(doc))\n",
        "  unique_doc = {}\n",
        "  for word in uniqued_list:\n",
        "    unique_doc[word] = doc.count(word)\n",
        "  tmp_data_set[list(tmp_tokenized_docs.keys())[list(tmp_tokenized_docs.values()).index(doc)]] = unique_doc\n",
        "\n",
        "# tmp_bigdataset = {}\n",
        "for postings_list in tmp_data_set.values():\n",
        "  for word in postings_list.keys():\n",
        "    if word in tmp_all_dict.keys():\n",
        "      tmp_all_dict[word] += postings_list[word]\n",
        "    else:\n",
        "      tmp_all_dict[word] = postings_list[word]\n",
        "T = len(tmp_all_dict.values())\n",
        "print(\"Number of T : \", T)\n",
        "print(\"b = 1/2 and k = 30 : \", 30 * T ** 0.5)\n",
        "print(\"b = 1/2 and k = 100 : \", 100 * T ** 0.5)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of T :  70864\n",
            "b = 1/2 and k = 30 :  7986.087903347921\n",
            "b = 1/2 and k = 100 :  26620.293011159738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD8ZBDOqIwQ4"
      },
      "source": [
        "This looks about right too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "vY7FH98-fw_m",
        "outputId": "2247eeb2-865a-4e36-e242-e92726384538"
      },
      "source": [
        "list(pp_docs.items())[0][1]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'مسلم مجدمی در گفت\\u200cوگو با خبرنگار ورزشی خبرگزاری فارس در مورد شکست 3 بر صفر نفت\\nمسجدسلیمان مقابل سپاهان اظهار داشت : فکر نمی\\u200cکردیم این اتفاق بیفتد . زود گل\\nخوردیم و غافلگیر شدیم . پیش از شروع مسابقات تنها یک هفته تمرین کرده\\u200cبودیم و با\\nاین شرایط نمی\\u200cتوان تیم ما را با بقیه تیم\\u200cها مقایسه کرد . برخی تیم\\u200cها حتی بیشتر\\nاز 10 بازی تدارکاتی انجام دادند و بدنسازی به موقعی داشتند .\\n وی تصریح کرد : پیش از این بازی هم وضعیت\\u200c ما مشخص نبود . در زمین خودمان بازی\\nنمی\\u200cکردیم و همه این اتفاقات دست به دست هم داد مشکلات زیادی داشته باشیم . با این\\nحال فوتبال برد و باخت دارد و تلاش می\\u200cکنیم تا به امید خدا این نتیجه را در\\nبازی\\u200cهای بعدی جبران کنیم .\\n مجدمی در مورد زیر کشت رفتن و شخم زدن زمین ورزشگاه بهنام پیش از بازی با سپاهان\\nعنوان کرد : \\u200c ما برای تمرین به مسجدسلیمان رفتیم اما دیدیم این اتفاق برای زمین\\nافتاده است . نمی\\u200cدانم کار چه کسی بوده اما واقعا به ما ضربه زد . راه زیادی را از\\nتهران به اهواز و سپس به مسجدسلیمان رفتیم . می\\u200cخواستیم آماده بازی با سپاهان شویم\\nاما دیدیم شرایط زمین خوب نیست . اگر در آن تمرین هم می\\u200cکردیم بدتر می\\u200cشد . حتی یک\\nجلسه هم پشت دروازه تمرین کردیم اما مجبور شدیم به اهواز برگردیم . با این شرایط\\nنمی\\u200cدانم بعد از چند هفته زمین خانگی ما آماده مسابقه می\\u200cشود .\\n هافبک تیم فوتبال نفت مسجدسلیمان در مورد اعتراض [ باشگاه\\nپرسپولیس ]( https :// www . farsnews . ir / special / باشگاه پرسپولیس ) به تغییر محل زمین\\nبازی و اینکه افشین پیروانی اعلام کرد این اتفاق برای دادن امتیاز به تیم سپاهان\\nبود ، گفت : فکر نمی\\u200cکنم در این شرایط تیمی بخواهد به تیم دیگری امتیاز بدهد . این\\nحرف\\u200cها زشت است . بچه\\u200cهای ما با \\n[ شرافت ]( https :// search . farsnews . ir /? q = شرافت & o = on ) بازی کردند . باید چه کار\\nمی\\u200cکردیم ؟ مجبور\\u200cبودیم برای بازی به اهواز برویم . کجا باید بازی می\\u200cکردیم ؟ ما هم\\nدوست داشتیم در این شرایط از میزبانی خودمان استفاده کنیم . در هفته اول یک امتیاز\\nاز پیکان گرفتیم و در بازی با سپاهان هم اگر حتی یک امتیاز می\\u200cگرفتیم نتیجه \\n[ خوبی ]( https :// search . farsnews . ir /? q = خوبی & o = on )\\u200cبود . این حرف\\u200cها برای یک باشگاه\\nفوتبال قشنگ نیست .\\n مجدمی در مورد استعفای مدیرعامل باشگاه نفت مسجدسلیمان اظهار داشت : من از این\\nمسئله مطمئن نیستم . دیروز عصر سر تمرین هم خبری نبود اما صددرصد هرچه تغییرات\\nزیاد باشد تیم را اذیت می\\u200cکند . ما تابع تصمیمات باشگاه هستیم و امیدوارم هر\\nاتفاقی می\\u200cافتد به صلاح تیم باشد .\\n وی در مورد بازی هفته آینده با تیم بحران زده پدیده در مشهد ، گفت : قطعا در این\\nبازی تلاش می\\u200cکنیم که بتوانیم نتیجه بگیریم . تیم خوبی داریم و باید به خودمان\\nنگاه کنیم . مقابل پیکان بازی خوبی انجام دادیم . مقابل سپاهان هم نباید اینطور\\nشکست می\\u200cخوردیم اما غافلگیر شدیم . پدیده هم شرایط خوبی ندارد و بعضی اوقات تعویض\\nمربیان به نفع حریف می\\u200cشود . در هر حال تمام تلاش\\u200cمان این است که با دست پر از\\nمشهد برگردیم .\\n انتهای پیام /'"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqXi4qEk0IbZ"
      },
      "source": [
        "Now let's tokenize each document.\n",
        "For tokenization I used [hazm](https://github.com/sobhe/hazm/tree/master/hazm) library. Tokenization means spliting sentences to words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "50dX9gy7y76a",
        "outputId": "b1a149c4-8561-4d84-991d-4915563ef714"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 49.7 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=fda39d88c4a67aa392acafa54010e6f8308fdc2dcd6dcfa2a6916ebf6e9ddfcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154580 sha256=5666da21ad3c68f73a667d8fcb8cb24da77837bd8a8d2d1af143c75c4dbded54\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.4.5\n",
            "    Uninstalling nltk-3.4.5:\n",
            "      Successfully uninstalled nltk-3.4.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "parsivar 0.2.3 requires nltk==3.4.5, but you have nltk 3.3 which is incompatible.\u001b[0m\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YARaiKNqCfAX"
      },
      "source": [
        "from hazm import word_tokenize\n",
        "def f(v):\n",
        "  return word_tokenize(v)\n",
        "\n",
        "tokenized_docs = {k: f(v) for k, v in pp_docs.items()}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsCuoslYMOvo",
        "outputId": "bb5d5f2b-57b9-458a-ba63-45954351d2bb"
      },
      "source": [
        "list(tokenized_docs.items())[0][1][:5]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['مسلم', 'مجدمی', 'در', 'گفت\\u200cوگو', 'با']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgFUJ9ltLiGW"
      },
      "source": [
        "from hazm import Stemmer\n",
        "stemmer = Stemmer()\n",
        "# stemmed_doc = tokenized_docs.copy()\n",
        "for doc in tokenized_docs.values():\n",
        "  for word in doc:\n",
        "    word = stemmer.stem(word)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnOBEWs_MNIt",
        "outputId": "0960423b-5752-4b04-cea4-59050310d259"
      },
      "source": [
        "list(tokenized_docs.items())[1][1][:5]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['به', 'گزارش', 'خبرنگار', 'ورزشی', 'خبرگزاری']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY324JpMvqBW"
      },
      "source": [
        "For checking the correction of zipf's law, first we calculate an example of $c_i = \\frac{c_1}{i}$ and then we check it another time after removing the stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pexX1KHa88H2",
        "outputId": "106cf1b2-17c5-43fb-aaa0-f2a8824376aa"
      },
      "source": [
        "all_dict = {}\n",
        "\n",
        "data_set = {}\n",
        "for doc in tokenized_docs.values():\n",
        "  uniqued_list = list(dict.fromkeys(doc))\n",
        "  unique_doc = {}\n",
        "  for word in uniqued_list:\n",
        "    unique_doc[word] = doc.count(word)\n",
        "  data_set[list(tokenized_docs.keys())[list(tokenized_docs.values()).index(doc)]] = unique_doc\n",
        "\n",
        "# tmp_bigdataset = {}\n",
        "for postings_list in data_set.values():\n",
        "  for word in postings_list.keys():\n",
        "    if word in all_dict.keys():\n",
        "      all_dict[word] += postings_list[word]\n",
        "    else:\n",
        "      all_dict[word] = postings_list[word]\n",
        "\n",
        "import operator\n",
        "sorted_x = sorted(all_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
        "print(type(sorted_x))\n",
        "print(sorted_x[:5])\n",
        "sum_of_all = sum(list(all_dict.values()))\n",
        "c1 = sorted_x[0][1]\n",
        "c5 = sorted_x[4][1]\n",
        "print(\"c5 : \", c5)\n",
        "print(\"c1 / 5 : \", c1 / 5)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "[('.', 99154), ('،', 76158), (':', 34566), ('/', 31993), (')', 19522)]\n",
            "c5 :  19522\n",
            "c1 / 5 :  19830.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJtA7R89RIiA"
      },
      "source": [
        "That looks kinda true.\n",
        "\n",
        "\n",
        "For last step of preprocess, we need to remove stop words of our data. We can get a list of stop words from hazm library and then remove stop words. Removing stop words help us not to process unnecessary data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hkcuOlpMvOz",
        "outputId": "c52a805e-a430-42f2-aa87-d1b0733a9079"
      },
      "source": [
        "from hazm import stopwords_list\n",
        "stopwords = stopwords_list()\n",
        "stopwords += ['.', '،', ':', '/', ')', '(', '%', '//', '[', ']'] \n",
        "stopwords[20:25]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['وی', 'شد', 'دارد', 'ما', 'اما']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_9jttR5RjoM"
      },
      "source": [
        "for title, doc in tokenized_docs.items():\n",
        "  tokenized_docs[title] = [word for word in doc if word not in stopwords]\n",
        "\n",
        "  # for word in stopwords:\n",
        "      # print(word)\n",
        "      # doc = list(filter((word).__ne__, doc))\n",
        "  # print(doc[:5])\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UX530xBDAgM"
      },
      "source": [
        "Now let's check zipf's law again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPEvoAm9C_RI",
        "outputId": "f0ae5fdf-cec8-4382-97ab-5a224fd77470"
      },
      "source": [
        "all_dict = {}\n",
        "\n",
        "data_set = {}\n",
        "for doc in tokenized_docs.values():\n",
        "  uniqued_list = list(dict.fromkeys(doc))\n",
        "  unique_doc = {}\n",
        "  for word in uniqued_list:\n",
        "    unique_doc[word] = doc.count(word)\n",
        "  data_set[list(tokenized_docs.keys())[list(tokenized_docs.values()).index(doc)]] = unique_doc\n",
        "\n",
        "# tmp_bigdataset = {}\n",
        "for postings_list in data_set.values():\n",
        "  for word in postings_list.keys():\n",
        "    if word in all_dict.keys():\n",
        "      all_dict[word] += postings_list[word]\n",
        "    else:\n",
        "      all_dict[word] = postings_list[word]\n",
        "\n",
        "import operator\n",
        "sorted_x = sorted(all_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
        "print(type(sorted_x))\n",
        "print(sorted_x[:5])\n",
        "sum_of_all = sum(list(all_dict.values()))\n",
        "c1 = sorted_x[0][1]\n",
        "c5 = sorted_x[4][1]\n",
        "print(\"c5 : \", c5)\n",
        "print(\"c1 / 5 : \", c1 / 5)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "[('ir', 10863), ('https', 10717), ('farsnews', 9587), ('ایران', 9462), ('کشور', 8665)]\n",
            "c5 :  8665\n",
            "c1 / 5 :  2172.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_fFIJB4EVDd"
      },
      "source": [
        "This one looks kinda true too. The reason is zipf's law is not about stopwords. It is about statistics and it says when we have a lot of data the most frequent occurs $cf_1$ times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj4qvvXLSw1g",
        "outputId": "2fd50652-f1c7-4f6a-ce09-b751cfe21e6d"
      },
      "source": [
        "list(tokenized_docs.items())[1][1][:5]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['گزارش', 'خبرنگار', 'ورزشی', 'خبرگزاری', 'فارس']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZS4QXDPDxK"
      },
      "source": [
        "positional_index = {}\n",
        "for doc in tokenized_docs.values():\n",
        "  pos_indx = {}\n",
        "  uniqued_list = list(dict.fromkeys(doc))\n",
        "  for word in uniqued_list:\n",
        "    indices = [i for i, x in enumerate(doc) if x == word]\n",
        "    pos_indx[word] = indices\n",
        "  positional_index[list(tokenized_docs.keys())[list(tokenized_docs.values()).index(doc)]] = pos_indx\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEzV1v_gQLcu",
        "outputId": "e8ce4527-bce6-4b1c-d0f0-a54b0b9d8856"
      },
      "source": [
        "list(list(positional_index.items())[0][1].items())[:5] "
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('مسلم', [0]),\n",
              " ('مجدمی', [1, 62, 207]),\n",
              " ('گفت\\u200cوگو', [2]),\n",
              " ('خبرنگار', [3]),\n",
              " ('ورزشی', [4])]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT3OLvWffcfN"
      },
      "source": [
        "Now that we have preprocessed our data, it's time to create posting lists dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1bx8nA5fcGI"
      },
      "source": [
        "# keys = list(tokenized_docs.keys())\n",
        "# data_set = {x:None' for x in keys}\n",
        "data_set = {}\n",
        "boolean_data_set = {}\n",
        "# data_set = tokenized_docs.copy()\n",
        "for doc in tokenized_docs.values():\n",
        "  new_doc = []\n",
        "  # tokenized_docs.keys())[list(tokenized_docs.values()).index(doc)] = []\n",
        "  for i, word in enumerate(doc):\n",
        "    if i == len(doc) - 1:\n",
        "      continue\n",
        "    new_doc.append((doc[i],doc[i+1]))\n",
        "  unique_bool_ls = list(dict.fromkeys(new_doc))\n",
        "  uniqued_list = list(dict.fromkeys(doc))\n",
        "  unique_doc = {}\n",
        "  unique_bool = {}\n",
        "  for word in unique_bool_ls:\n",
        "    unique_bool[word] = new_doc.count(word)\n",
        "  for word in uniqued_list:\n",
        "    unique_doc[word] = doc.count(word)\n",
        "  data_set[list(tokenized_docs.keys())[list(tokenized_docs.values()).index(doc)]] = unique_doc\n",
        "  boolean_data_set[list(tokenized_docs.keys())[list(tokenized_docs.values()).index(doc)]] = unique_bool\n",
        "  "
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjkzL0AwN2Xv",
        "outputId": "b7cd3896-e349-4a91-d71f-4f63c243956e"
      },
      "source": [
        "list(list(boolean_data_set.items())[0][1].items())[:5] "
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('مسلم', 'مجدمی'), 1),\n",
              " (('مجدمی', 'گفت\\u200cوگو'), 1),\n",
              " (('گفت\\u200cوگو', 'خبرنگار'), 1),\n",
              " (('خبرنگار', 'ورزشی'), 1),\n",
              " (('ورزشی', 'خبرگزاری'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG5mG5J_fahu",
        "outputId": "f534baa5-1ba9-457d-edab-39e55d1d6358"
      },
      "source": [
        "list(list(data_set.items())[0][1].items())[:5] # postings list"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('مسلم', 1), ('مجدمی', 3), ('گفت\\u200cوگو', 1), ('خبرنگار', 1), ('ورزشی', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FTad4TnPoQfe",
        "outputId": "2dfcf007-0bbd-4742-99c0-5f29a0733afd"
      },
      "source": [
        "list(data_set.items())[0][0] # title"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'واکنش بازیکن نفت مسجدسلیمان به صحبت\\u200cهای پیروانی:با شرافت بازی کردیم/ این حرف ها زشت است'"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrNCVSqAoiwN"
      },
      "source": [
        "Now let's create a bigger postings list for all of the words in all documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko2UMfaHoaUS"
      },
      "source": [
        "bigdataset = {}\n",
        "for postings_list in data_set.values():\n",
        "  for word in postings_list.keys():\n",
        "    if word in bigdataset.keys():\n",
        "      bigdataset[word] += postings_list[word]\n",
        "    else:\n",
        "      bigdataset[word] = postings_list[word]\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VlY4mmopu6b",
        "outputId": "d9f25335-5f9d-4fe5-a637-0e4904e7d97a"
      },
      "source": [
        "list(bigdataset.items())[80:90]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('جلسه', 2154),\n",
              " ('دروازه', 214),\n",
              " ('کردیم', 837),\n",
              " ('مجبور', 124),\n",
              " ('برگردیم', 20),\n",
              " ('خانگی', 346),\n",
              " ('مسابقه', 896),\n",
              " ('هافبک', 163),\n",
              " ('اعتراض', 223),\n",
              " ('باشگاه', 2011)]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvEPi_iWj8Fu"
      },
      "source": [
        "Next step is to create a boolean retrieval model. User's query can be single worded or multi worded.\n",
        "\n",
        "At first I chose the best doc by the order of : 1- Which document contains more different words? 2- Which document has more total count of words?\n",
        "\n",
        "This approach had a little problem, when I searched for \"دانشگاه امیرکبیر\" the result was the document that had about 40 counts of \"دانشگاه\" and 1 count of \"امیرکبیر\", while the answer should have about equal amount of both words in it. So I changed the retrieval model to a model that retrieve a document that has has more count for each of words, except for all of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnxJ9pi-HBpM"
      },
      "source": [
        "def give_doc(inp_str):\n",
        "  global positional_index, data_set, boolean_data_set, tokenized_docs, pp_docs\n",
        "  words = inp_str.split()\n",
        "  if len(words) == 1:\n",
        "    word = words[0]\n",
        "    output_list = []\n",
        "    for title in data_set.keys():\n",
        "      doc = data_set[title]\n",
        "      if word in doc.keys():\n",
        "        output_list.append(title)\n",
        "    print(f\"First five titles of the retrieved doc : {output_list[:5]}\")\n",
        "  \n",
        "  elif len(words) == 2:\n",
        "    word = (words[0], words[1])\n",
        "    # print(word)\n",
        "    output_list = {}\n",
        "    for title in boolean_data_set.keys():\n",
        "      doc = boolean_data_set[title]\n",
        "      if word in doc.keys():\n",
        "        output_list[title] = \"Pair\"\n",
        "    if output_list:\n",
        "      print(f\"First five titles of the retrieved doc : {list(output_list.items())[:5]}\")\n",
        "    else:\n",
        "      for title in data_set.keys():\n",
        "        doc = data_set[title]\n",
        "        if word[0] in doc.keys():\n",
        "          output_list[title] = word[0]\n",
        "        if word[1] in doc.keys():\n",
        "          output_list[title] = word[1]\n",
        "      print(f\"First five titles of the retrieved doc : {list(output_list.items())[:5]}\")\n",
        "\n",
        "  elif len(words) > 2:\n",
        "    word = inp_str\n",
        "    # word = [word +\" \"+ words[i] for i in words]\n",
        "    # print(word)\n",
        "    output_list = {}\n",
        "    for title in pp_docs.keys():\n",
        "      # output_list = {}\n",
        "      doc = pp_docs[title]\n",
        "      if word in doc:\n",
        "        output_list[title] = \"all\"\n",
        "    if output_list:\n",
        "        print(f\"First five titles of the retrieved doc : {list(output_list.items())[:5]}\")\n",
        "    else:\n",
        "      pairs = []\n",
        "      for i, word in enumerate(words):\n",
        "        if i == len(words) - 1:\n",
        "          continue\n",
        "        else:\n",
        "          pairs.append((words[i], words[i+1]))\n",
        "      for pair in pairs:  \n",
        "        for title in boolean_data_set.keys():\n",
        "          doc = boolean_data_set[title]\n",
        "          if pair in doc.keys():\n",
        "            output_list[title] = pair\n",
        "      if output_list:\n",
        "          print(f\"First five titles of the retrieved doc : {list(output_list.items())[:5]}\")\n",
        "      else:\n",
        "        for word in words:  \n",
        "          for title in data_set.keys():\n",
        "            doc = data_set[title]\n",
        "            if word in doc.keys():\n",
        "              output_list[title] = word\n",
        "        print(f\"First five titles of the retrieved doc : {list(output_list.items())[:5]}\")\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oP3dSphp3j5",
        "outputId": "0d61680f-f4f7-4f14-a767-da2d66bb3d11"
      },
      "source": [
        "query = \"\"\n",
        "while query != 'q':\n",
        "  query = input()\n",
        "  if query == 'q':\n",
        "    break\n",
        "  give_doc(query)\n",
        " "
      ],
      "execution_count": 88,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "بین‌الملل\n",
            "First five titles of the retrieved doc : ['توضیحات مسؤول مسابقات لیگ یک درباره شایعه سکته ناظر بازی', 'اعلام آخرین اقدامات تراکتور برای بازشدن پنجره/ احتمال استفاده از بازیکنان جدید برابر گل\\u200cگهر', 'گزارش تمرین پرسپولیس| روحیه شاد قبل از مصاف با الهلال/ پا به توپ شدن گل\\u200cمحمدی و باقری', 'واکنش نصیرزاده به اظهارات مالک ماشین سازی: به جای فرافکنی به تعهداتتان عمل کنید', 'باج بن سلمان به انگلیس و قطر/ پروژه فوتبالی آل سعود چگونه رقم خورد؟']\n",
            "دانشگاه امیرکبیر\n",
            "First five titles of the retrieved doc : [('نامه جمعی از اساتید و متخصصان/ آقای رئیس\\u200cجمهور در گام دوم انقلاب به داد «مدیریت» در کشور برسید', 'Pair'), ('نامه ۸ بسیج دانشجویی دانشگاه\\u200cهای تهران به معاون اول رئیس جمهور', 'Pair'), ('بزرگداشت شهدای مسجد قندوز در مقابل کنسولگری افغانستان/ آمریکا و آل سعود مقصران اصلی جنایت در افغانستان', 'Pair'), ('امروز محیط دانشگاه\\u200cهای ما عرصه دفاع مقدس است', 'Pair'), ('باید برای ثبت نقش دانشگاهیان در دوران دفاع مقدس کار تحقیقاتی صورت گیرد', 'Pair')]\n",
            "دانشگاه صنعتی امیرکبیر\n",
            "First five titles of the retrieved doc : [('باید برای ثبت نقش دانشگاهیان در دوران دفاع مقدس کار تحقیقاتی صورت گیرد', 'all'), ('دفترچه راهنمای آزمون استخدامی دانشگاه\\u200cها برای بار چهارم اصلاح شد/تمدید مجدد مهلت ثبت\\u200cنام', 'all')]\n",
            "سازمان ملل متحد\n",
            "First five titles of the retrieved doc : [('گزارش نظارت میدانی نمایندگان از مرزهای شمال\\u200cغرب به کمیسیون امنیت ملی', 'all'), ('محسن اسلامی مدیرکل دفتر امور سیاسی وزارت کشور شد', 'all'), ('اعتراض هیات پارلمانی ایران به نماینده منصور هادی در اجلاس تغییرات آب و هوایی/ همکاری دولت مستعفی یمن با ائتلاف سعودی علیه یمنی\\u200cها', 'all'), ('هیئت پارلمانی ایران به رم سفر کرد', 'all'), ('۳ راهبرد دولت آیت\\u200cالله رئیسی برای شکست تحریم\\u200cها', 'all')]\n",
            "ژیمناستیک\n",
            "First five titles of the retrieved doc : ['خیرخواه: برخی به دنبال فلج کردن ژیمناستیک هستند/ با بایکوت فدراسیون موفقیت\\u200cها بیشتر شد', 'هشدار هیات ژیمناستیک تهران در خصوص سالن\\u200cهای مختلط و اقدامات غیراخلاقی', 'دبیر مجمع فدراسیون ژیمناستیک مشخص شد', 'ثبت نام ۱۳ نامزد برای پست ریاست فدراسیون ژیمناستیک + اسامی', 'جزییات تعطیلی ورزش ایران تا پایان تیرماه+ تصویر']\n",
            "واکسن آسترازنکا\n",
            "First five titles of the retrieved doc : [('محموله ۱.۴ میلیون دوزی واکسن کرونا وارد کشور شد', 'Pair'), ('مهم\\u200cترین سلاح مبارزه با کرونا', 'Pair'), ('نکاتی که باید در مورد واکسیناسیون کرونا بدانیم ', 'Pair'), ('واکسن\\u200cهای کرونا با چه داروهایی تداخل دارند؟', 'Pair'), ('امکان ایجاد لخته خون در واکسن  آسترازنکا چقدر است؟', 'Pair')]\n",
            "q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThgVu81ks27F"
      },
      "source": [
        "These answers are mostly correct because retrieved docs have exactly the searched query in them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxTH5kX2E2N5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}